{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uSDX-X14zr0m"},"outputs":[],"source":["from __future__ import print_function\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from keras.preprocessing import sequence\n","from sklearn.preprocessing import label_binarize\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers import Convolution1D,MaxPooling1D, Flatten, Dense, Dropout, Activation, Embedding\n","from keras.layers import LSTM, SimpleRNN, GRU ,Bidirectional, CuDNNLSTM ,BatchNormalization ,Flatten, Attention\n","from keras.utils.np_utils import to_categorical\n","from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score, matthews_corrcoef, roc_curve, auc, roc_auc_score)\n","from sklearn import metrics\n","from sklearn.preprocessing import Normalizer , LabelEncoder , StandardScaler\n","import h5py\n","from keras import callbacks\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","import math\n","import time\n","from itertools import cycle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19652,"status":"ok","timestamp":1665140721877,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"pUYmBOw7zwQp","outputId":"2e491661-bc72-4385-9e7e-fbb43b8ed77a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1uHspEJz0Ty"},"outputs":[],"source":["data = pd.read_csv(r'/content/drive/MyDrive/MSCAD/MSCAD.csv', header=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JoST1dPSgdV"},"outputs":[],"source":["c=[ 3, 7, 12, 13, 15, 16, 22, 23, 24, 25, 33, 34, 36, 39, 50, 52, 56, 57, 66]\n","data=data[data.columns[c]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665043821536,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"keWgRvicTzeb","outputId":"52578008-aa28-440d-99b2-d9788a5f19b8"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-27702fbb-1e8d-4e77-86ac-a8eb92312c73\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>'TotLen Fwd Pkts'</th>\n","      <th>'Fwd Pkt Len Mean'</th>\n","      <th>'Bwd Pkt Len Std'</th>\n","      <th>'Flow Byts/s'</th>\n","      <th>'Flow IAT Mean'</th>\n","      <th>'Flow IAT Std'</th>\n","      <th>'Fwd IAT Max'</th>\n","      <th>'Fwd IAT Min'</th>\n","      <th>'Bwd IAT Tot'</th>\n","      <th>'Bwd IAT Mean'</th>\n","      <th>'Fwd Pkts/s'</th>\n","      <th>'Bwd Pkts/s'</th>\n","      <th>'Pkt Len Max'</th>\n","      <th>'Pkt Len Var'</th>\n","      <th>'Fwd Seg Size Avg'</th>\n","      <th>'Subflow Fwd Pkts'</th>\n","      <th>'Init Bwd Win Byts'</th>\n","      <th>'Fwd Act Data Pkts'</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>110</td>\n","      <td>55.0000</td>\n","      <td>168.5995</td>\n","      <td>3.208169e+05</td>\n","      <td>2.530000e+02</td>\n","      <td>1.595306e+02</td>\n","      <td>487</td>\n","      <td>487</td>\n","      <td>1518</td>\n","      <td>379.5</td>\n","      <td>1317.5231</td>\n","      <td>3293.8076</td>\n","      <td>377</td>\n","      <td>17797.5536</td>\n","      <td>55.0000</td>\n","      <td>2</td>\n","      <td>46</td>\n","      <td>1</td>\n","      <td>Brute_Force</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>168</td>\n","      <td>42.0000</td>\n","      <td>744.4771</td>\n","      <td>7.916525e+05</td>\n","      <td>5.358182e+02</td>\n","      <td>7.749130e+02</td>\n","      <td>2675</td>\n","      <td>677</td>\n","      <td>5894</td>\n","      <td>842.0</td>\n","      <td>678.6563</td>\n","      <td>1357.3125</td>\n","      <td>1460</td>\n","      <td>396851.0769</td>\n","      <td>42.0000</td>\n","      <td>4</td>\n","      <td>54</td>\n","      <td>1</td>\n","      <td>Brute_Force</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.720000e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>3676.4706</td>\n","      <td>3676.4706</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>1</td>\n","      <td>4106</td>\n","      <td>0</td>\n","      <td>Brute_Force</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>322</td>\n","      <td>80.5000</td>\n","      <td>750.2544</td>\n","      <td>1.821524e+06</td>\n","      <td>2.373636e+02</td>\n","      <td>2.376124e+02</td>\n","      <td>1368</td>\n","      <td>367</td>\n","      <td>2611</td>\n","      <td>373.0</td>\n","      <td>1531.9801</td>\n","      <td>3063.9602</td>\n","      <td>1460</td>\n","      <td>396786.3077</td>\n","      <td>80.5000</td>\n","      <td>4</td>\n","      <td>54</td>\n","      <td>1</td>\n","      <td>Brute_Force</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.940000e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>3401.3605</td>\n","      <td>3401.3605</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>1</td>\n","      <td>4106</td>\n","      <td>0</td>\n","      <td>Brute_Force</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128794</th>\n","      <td>517</td>\n","      <td>172.3333</td>\n","      <td>0.0000</td>\n","      <td>7.499057e+03</td>\n","      <td>1.723550e+04</td>\n","      <td>2.928714e+04</td>\n","      <td>7697</td>\n","      <td>304</td>\n","      <td>60845</td>\n","      <td>60845.0</td>\n","      <td>43.5148</td>\n","      <td>29.0099</td>\n","      <td>517</td>\n","      <td>44548.1667</td>\n","      <td>172.3333</td>\n","      <td>3</td>\n","      <td>65535</td>\n","      <td>1</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>128795</th>\n","      <td>1277</td>\n","      <td>638.5000</td>\n","      <td>958.8368</td>\n","      <td>4.157784e+04</td>\n","      <td>2.110900e+04</td>\n","      <td>3.651164e+04</td>\n","      <td>63327</td>\n","      <td>63327</td>\n","      <td>58</td>\n","      <td>58.0</td>\n","      <td>31.5821</td>\n","      <td>31.5821</td>\n","      <td>1356</td>\n","      <td>531444.2000</td>\n","      <td>638.5000</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>128796</th>\n","      <td>517</td>\n","      <td>172.3333</td>\n","      <td>0.0000</td>\n","      <td>3.717793e+03</td>\n","      <td>2.781220e+04</td>\n","      <td>3.653110e+04</td>\n","      <td>71498</td>\n","      <td>254</td>\n","      <td>135929</td>\n","      <td>67964.5</td>\n","      <td>21.5733</td>\n","      <td>21.5733</td>\n","      <td>517</td>\n","      <td>38184.1429</td>\n","      <td>172.3333</td>\n","      <td>3</td>\n","      <td>261</td>\n","      <td>1</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>128797</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.4472</td>\n","      <td>4.620000e-02</td>\n","      <td>3.606975e+06</td>\n","      <td>4.534670e+06</td>\n","      <td>10100000</td>\n","      <td>10100000</td>\n","      <td>21600000</td>\n","      <td>5410462.0</td>\n","      <td>0.0924</td>\n","      <td>0.2310</td>\n","      <td>1</td>\n","      <td>0.1250</td>\n","      <td>0.0000</td>\n","      <td>2</td>\n","      <td>257</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>128798</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.480230e+05</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>248023</td>\n","      <td>248023.0</td>\n","      <td>0.0000</td>\n","      <td>8.0638</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0</td>\n","      <td>1026</td>\n","      <td>0</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>128799 rows Ã— 19 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27702fbb-1e8d-4e77-86ac-a8eb92312c73')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-27702fbb-1e8d-4e77-86ac-a8eb92312c73 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-27702fbb-1e8d-4e77-86ac-a8eb92312c73');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        'TotLen Fwd Pkts'  'Fwd Pkt Len Mean'  'Bwd Pkt Len Std'  \\\n","0                     110             55.0000           168.5995   \n","1                     168             42.0000           744.4771   \n","2                       0              0.0000             0.0000   \n","3                     322             80.5000           750.2544   \n","4                       0              0.0000             0.0000   \n","...                   ...                 ...                ...   \n","128794                517            172.3333             0.0000   \n","128795               1277            638.5000           958.8368   \n","128796                517            172.3333             0.0000   \n","128797                  0              0.0000             0.4472   \n","128798                  0              0.0000             0.0000   \n","\n","        'Flow Byts/s'  'Flow IAT Mean'  'Flow IAT Std'  'Fwd IAT Max'  \\\n","0        3.208169e+05     2.530000e+02    1.595306e+02            487   \n","1        7.916525e+05     5.358182e+02    7.749130e+02           2675   \n","2        0.000000e+00     2.720000e+02    0.000000e+00              0   \n","3        1.821524e+06     2.373636e+02    2.376124e+02           1368   \n","4        0.000000e+00     2.940000e+02    0.000000e+00              0   \n","...               ...              ...             ...            ...   \n","128794   7.499057e+03     1.723550e+04    2.928714e+04           7697   \n","128795   4.157784e+04     2.110900e+04    3.651164e+04          63327   \n","128796   3.717793e+03     2.781220e+04    3.653110e+04          71498   \n","128797   4.620000e-02     3.606975e+06    4.534670e+06       10100000   \n","128798   0.000000e+00     2.480230e+05    0.000000e+00              0   \n","\n","        'Fwd IAT Min'  'Bwd IAT Tot'  'Bwd IAT Mean'  'Fwd Pkts/s'  \\\n","0                 487           1518           379.5     1317.5231   \n","1                 677           5894           842.0      678.6563   \n","2                   0              0             0.0     3676.4706   \n","3                 367           2611           373.0     1531.9801   \n","4                   0              0             0.0     3401.3605   \n","...               ...            ...             ...           ...   \n","128794            304          60845         60845.0       43.5148   \n","128795          63327             58            58.0       31.5821   \n","128796            254         135929         67964.5       21.5733   \n","128797       10100000       21600000       5410462.0        0.0924   \n","128798              0         248023        248023.0        0.0000   \n","\n","        'Bwd Pkts/s'  'Pkt Len Max'  'Pkt Len Var'  'Fwd Seg Size Avg'  \\\n","0          3293.8076            377     17797.5536             55.0000   \n","1          1357.3125           1460    396851.0769             42.0000   \n","2          3676.4706              0         0.0000              0.0000   \n","3          3063.9602           1460    396786.3077             80.5000   \n","4          3401.3605              0         0.0000              0.0000   \n","...              ...            ...            ...                 ...   \n","128794       29.0099            517     44548.1667            172.3333   \n","128795       31.5821           1356    531444.2000            638.5000   \n","128796       21.5733            517     38184.1429            172.3333   \n","128797        0.2310              1         0.1250              0.0000   \n","128798        8.0638              0         0.0000              0.0000   \n","\n","        'Subflow Fwd Pkts'  'Init Bwd Win Byts'  'Fwd Act Data Pkts'  \\\n","0                        2                   46                    1   \n","1                        4                   54                    1   \n","2                        1                 4106                    0   \n","3                        4                   54                    1   \n","4                        1                 4106                    0   \n","...                    ...                  ...                  ...   \n","128794                   3                65535                    1   \n","128795                   2                    0                    1   \n","128796                   3                  261                    1   \n","128797                   2                  257                    0   \n","128798                   0                 1026                    0   \n","\n","              Label  \n","0       Brute_Force  \n","1       Brute_Force  \n","2       Brute_Force  \n","3       Brute_Force  \n","4       Brute_Force  \n","...             ...  \n","128794       Normal  \n","128795       Normal  \n","128796       Normal  \n","128797       Normal  \n","128798       Normal  \n","\n","[128799 rows x 19 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1665140772756,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"o68URemF0NMQ","outputId":"19e56343-2233-4fe4-8674-3a80abb41b85"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Brute_Force    88502\n","Normal         28502\n","Port_Scan      11081\n","HTTP_DDoS        641\n","ICMP_Flood        45\n","Web_Crwling       28\n","Name: Label, dtype: int64"]},"metadata":{},"execution_count":5}],"source":["data[data.columns[18]].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0xa2Ohg2dLR"},"outputs":[],"source":["X = data.iloc[:,0:18]\n","Y = data['Label']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1665043854694,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"mBpFfLMt2o_2","outputId":"13312b2a-9e10-4946-d170-6b40dc73fa7d"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c3b66fc9-6d8f-4ce5-91a8-78118d82b14d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>'TotLen Fwd Pkts'</th>\n","      <th>'Fwd Pkt Len Mean'</th>\n","      <th>'Bwd Pkt Len Std'</th>\n","      <th>'Flow Byts/s'</th>\n","      <th>'Flow IAT Mean'</th>\n","      <th>'Flow IAT Std'</th>\n","      <th>'Fwd IAT Max'</th>\n","      <th>'Fwd IAT Min'</th>\n","      <th>'Bwd IAT Tot'</th>\n","      <th>'Bwd IAT Mean'</th>\n","      <th>'Fwd Pkts/s'</th>\n","      <th>'Bwd Pkts/s'</th>\n","      <th>'Pkt Len Max'</th>\n","      <th>'Pkt Len Var'</th>\n","      <th>'Fwd Seg Size Avg'</th>\n","      <th>'Subflow Fwd Pkts'</th>\n","      <th>'Init Bwd Win Byts'</th>\n","      <th>'Fwd Act Data Pkts'</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>110</td>\n","      <td>55.0000</td>\n","      <td>168.5995</td>\n","      <td>3.208169e+05</td>\n","      <td>2.530000e+02</td>\n","      <td>1.595306e+02</td>\n","      <td>487</td>\n","      <td>487</td>\n","      <td>1518</td>\n","      <td>379.5</td>\n","      <td>1317.5231</td>\n","      <td>3293.8076</td>\n","      <td>377</td>\n","      <td>17797.5536</td>\n","      <td>55.0000</td>\n","      <td>2</td>\n","      <td>46</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>168</td>\n","      <td>42.0000</td>\n","      <td>744.4771</td>\n","      <td>7.916525e+05</td>\n","      <td>5.358182e+02</td>\n","      <td>7.749130e+02</td>\n","      <td>2675</td>\n","      <td>677</td>\n","      <td>5894</td>\n","      <td>842.0</td>\n","      <td>678.6563</td>\n","      <td>1357.3125</td>\n","      <td>1460</td>\n","      <td>396851.0769</td>\n","      <td>42.0000</td>\n","      <td>4</td>\n","      <td>54</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.720000e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>3676.4706</td>\n","      <td>3676.4706</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>1</td>\n","      <td>4106</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>322</td>\n","      <td>80.5000</td>\n","      <td>750.2544</td>\n","      <td>1.821524e+06</td>\n","      <td>2.373636e+02</td>\n","      <td>2.376124e+02</td>\n","      <td>1368</td>\n","      <td>367</td>\n","      <td>2611</td>\n","      <td>373.0</td>\n","      <td>1531.9801</td>\n","      <td>3063.9602</td>\n","      <td>1460</td>\n","      <td>396786.3077</td>\n","      <td>80.5000</td>\n","      <td>4</td>\n","      <td>54</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.940000e+02</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>3401.3605</td>\n","      <td>3401.3605</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>1</td>\n","      <td>4106</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128794</th>\n","      <td>517</td>\n","      <td>172.3333</td>\n","      <td>0.0000</td>\n","      <td>7.499057e+03</td>\n","      <td>1.723550e+04</td>\n","      <td>2.928714e+04</td>\n","      <td>7697</td>\n","      <td>304</td>\n","      <td>60845</td>\n","      <td>60845.0</td>\n","      <td>43.5148</td>\n","      <td>29.0099</td>\n","      <td>517</td>\n","      <td>44548.1667</td>\n","      <td>172.3333</td>\n","      <td>3</td>\n","      <td>65535</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>128795</th>\n","      <td>1277</td>\n","      <td>638.5000</td>\n","      <td>958.8368</td>\n","      <td>4.157784e+04</td>\n","      <td>2.110900e+04</td>\n","      <td>3.651164e+04</td>\n","      <td>63327</td>\n","      <td>63327</td>\n","      <td>58</td>\n","      <td>58.0</td>\n","      <td>31.5821</td>\n","      <td>31.5821</td>\n","      <td>1356</td>\n","      <td>531444.2000</td>\n","      <td>638.5000</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>128796</th>\n","      <td>517</td>\n","      <td>172.3333</td>\n","      <td>0.0000</td>\n","      <td>3.717793e+03</td>\n","      <td>2.781220e+04</td>\n","      <td>3.653110e+04</td>\n","      <td>71498</td>\n","      <td>254</td>\n","      <td>135929</td>\n","      <td>67964.5</td>\n","      <td>21.5733</td>\n","      <td>21.5733</td>\n","      <td>517</td>\n","      <td>38184.1429</td>\n","      <td>172.3333</td>\n","      <td>3</td>\n","      <td>261</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>128797</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.4472</td>\n","      <td>4.620000e-02</td>\n","      <td>3.606975e+06</td>\n","      <td>4.534670e+06</td>\n","      <td>10100000</td>\n","      <td>10100000</td>\n","      <td>21600000</td>\n","      <td>5410462.0</td>\n","      <td>0.0924</td>\n","      <td>0.2310</td>\n","      <td>1</td>\n","      <td>0.1250</td>\n","      <td>0.0000</td>\n","      <td>2</td>\n","      <td>257</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>128798</th>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.000000e+00</td>\n","      <td>2.480230e+05</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>248023</td>\n","      <td>248023.0</td>\n","      <td>0.0000</td>\n","      <td>8.0638</td>\n","      <td>0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0</td>\n","      <td>1026</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>128799 rows Ã— 18 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3b66fc9-6d8f-4ce5-91a8-78118d82b14d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c3b66fc9-6d8f-4ce5-91a8-78118d82b14d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c3b66fc9-6d8f-4ce5-91a8-78118d82b14d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        'TotLen Fwd Pkts'  'Fwd Pkt Len Mean'  'Bwd Pkt Len Std'  \\\n","0                     110             55.0000           168.5995   \n","1                     168             42.0000           744.4771   \n","2                       0              0.0000             0.0000   \n","3                     322             80.5000           750.2544   \n","4                       0              0.0000             0.0000   \n","...                   ...                 ...                ...   \n","128794                517            172.3333             0.0000   \n","128795               1277            638.5000           958.8368   \n","128796                517            172.3333             0.0000   \n","128797                  0              0.0000             0.4472   \n","128798                  0              0.0000             0.0000   \n","\n","        'Flow Byts/s'  'Flow IAT Mean'  'Flow IAT Std'  'Fwd IAT Max'  \\\n","0        3.208169e+05     2.530000e+02    1.595306e+02            487   \n","1        7.916525e+05     5.358182e+02    7.749130e+02           2675   \n","2        0.000000e+00     2.720000e+02    0.000000e+00              0   \n","3        1.821524e+06     2.373636e+02    2.376124e+02           1368   \n","4        0.000000e+00     2.940000e+02    0.000000e+00              0   \n","...               ...              ...             ...            ...   \n","128794   7.499057e+03     1.723550e+04    2.928714e+04           7697   \n","128795   4.157784e+04     2.110900e+04    3.651164e+04          63327   \n","128796   3.717793e+03     2.781220e+04    3.653110e+04          71498   \n","128797   4.620000e-02     3.606975e+06    4.534670e+06       10100000   \n","128798   0.000000e+00     2.480230e+05    0.000000e+00              0   \n","\n","        'Fwd IAT Min'  'Bwd IAT Tot'  'Bwd IAT Mean'  'Fwd Pkts/s'  \\\n","0                 487           1518           379.5     1317.5231   \n","1                 677           5894           842.0      678.6563   \n","2                   0              0             0.0     3676.4706   \n","3                 367           2611           373.0     1531.9801   \n","4                   0              0             0.0     3401.3605   \n","...               ...            ...             ...           ...   \n","128794            304          60845         60845.0       43.5148   \n","128795          63327             58            58.0       31.5821   \n","128796            254         135929         67964.5       21.5733   \n","128797       10100000       21600000       5410462.0        0.0924   \n","128798              0         248023        248023.0        0.0000   \n","\n","        'Bwd Pkts/s'  'Pkt Len Max'  'Pkt Len Var'  'Fwd Seg Size Avg'  \\\n","0          3293.8076            377     17797.5536             55.0000   \n","1          1357.3125           1460    396851.0769             42.0000   \n","2          3676.4706              0         0.0000              0.0000   \n","3          3063.9602           1460    396786.3077             80.5000   \n","4          3401.3605              0         0.0000              0.0000   \n","...              ...            ...            ...                 ...   \n","128794       29.0099            517     44548.1667            172.3333   \n","128795       31.5821           1356    531444.2000            638.5000   \n","128796       21.5733            517     38184.1429            172.3333   \n","128797        0.2310              1         0.1250              0.0000   \n","128798        8.0638              0         0.0000              0.0000   \n","\n","        'Subflow Fwd Pkts'  'Init Bwd Win Byts'  'Fwd Act Data Pkts'  \n","0                        2                   46                    1  \n","1                        4                   54                    1  \n","2                        1                 4106                    0  \n","3                        4                   54                    1  \n","4                        1                 4106                    0  \n","...                    ...                  ...                  ...  \n","128794                   3                65535                    1  \n","128795                   2                    0                    1  \n","128796                   3                  261                    1  \n","128797                   2                  257                    0  \n","128798                   0                 1026                    0  \n","\n","[128799 rows x 18 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1665043941850,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"tThvMqkc3b_a","outputId":"65cd1c55-0639-4d5c-c411-dad75bc699f6"},"outputs":[{"data":{"text/plain":["0         Brute_Force\n","1         Brute_Force\n","2         Brute_Force\n","3         Brute_Force\n","4         Brute_Force\n","             ...     \n","128794         Normal\n","128795         Normal\n","128796         Normal\n","128797         Normal\n","128798         Normal\n","Name: Label, Length: 128799, dtype: object"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISPRi9Mt3d3c"},"outputs":[],"source":["scaler = StandardScaler().fit(X)\n","trainX = scaler.transform(X)\n","# summarize transformed data\n","np.set_printoptions(precision=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31tf2Ky836jp"},"outputs":[],"source":["label=LabelEncoder()\n","Y=label.fit_transform(Y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":899,"status":"ok","timestamp":1665043955406,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"BUBMK_aaUeBN","outputId":"72a87de3-8034-47f3-f27a-4282055604af"},"outputs":[{"data":{"text/plain":["array([0, 0, 0, ..., 3, 3, 3])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqVQWWCVFkOC"},"outputs":[],"source":["df=pd.DataFrame(trainX)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":818,"status":"ok","timestamp":1665043995204,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"pCqYxl9aWwa7","outputId":"2baa21a9-2de7-43da-c96f-a54efdc42c0f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c658fe50-199d-4bbb-b01a-c0ffa1596785\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.013860</td>\n","      <td>-0.002638</td>\n","      <td>-0.297168</td>\n","      <td>0.263236</td>\n","      <td>-0.201793</td>\n","      <td>-0.223605</td>\n","      <td>-0.262023</td>\n","      <td>-0.170951</td>\n","      <td>-0.191990</td>\n","      <td>-0.144498</td>\n","      <td>0.421145</td>\n","      <td>0.523996</td>\n","      <td>-0.264379</td>\n","      <td>-0.344355</td>\n","      <td>-0.002638</td>\n","      <td>-0.019815</td>\n","      <td>-0.340588</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.013832</td>\n","      <td>-0.120727</td>\n","      <td>1.313647</td>\n","      <td>1.129595</td>\n","      <td>-0.201762</td>\n","      <td>-0.223471</td>\n","      <td>-0.261847</td>\n","      <td>-0.170926</td>\n","      <td>-0.191751</td>\n","      <td>-0.144392</td>\n","      <td>0.133563</td>\n","      <td>0.027806</td>\n","      <td>1.245935</td>\n","      <td>0.664355</td>\n","      <td>-0.120727</td>\n","      <td>-0.018500</td>\n","      <td>-0.338914</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.201791</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.192073</td>\n","      <td>-0.144585</td>\n","      <td>1.483008</td>\n","      <td>0.622046</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.020473</td>\n","      <td>0.509332</td>\n","      <td>-0.017579</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.013757</td>\n","      <td>0.228998</td>\n","      <td>1.329807</td>\n","      <td>3.024606</td>\n","      <td>-0.201795</td>\n","      <td>-0.223588</td>\n","      <td>-0.261952</td>\n","      <td>-0.170967</td>\n","      <td>-0.191930</td>\n","      <td>-0.144500</td>\n","      <td>0.517681</td>\n","      <td>0.465102</td>\n","      <td>1.245935</td>\n","      <td>0.664182</td>\n","      <td>0.228998</td>\n","      <td>-0.018500</td>\n","      <td>-0.338914</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.201788</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.192073</td>\n","      <td>-0.144585</td>\n","      <td>1.359169</td>\n","      <td>0.551555</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.020473</td>\n","      <td>0.509332</td>\n","      <td>-0.017579</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128794</th>\n","      <td>-0.013662</td>\n","      <td>1.063190</td>\n","      <td>-0.768766</td>\n","      <td>-0.313283</td>\n","      <td>-0.199960</td>\n","      <td>-0.217234</td>\n","      <td>-0.261441</td>\n","      <td>-0.170975</td>\n","      <td>-0.188745</td>\n","      <td>-0.130657</td>\n","      <td>-0.152341</td>\n","      <td>-0.312547</td>\n","      <td>-0.069140</td>\n","      <td>-0.273168</td>\n","      <td>1.063190</td>\n","      <td>-0.019158</td>\n","      <td>13.368871</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>128795</th>\n","      <td>-0.013292</td>\n","      <td>5.297738</td>\n","      <td>1.913243</td>\n","      <td>-0.250576</td>\n","      <td>-0.199541</td>\n","      <td>-0.215654</td>\n","      <td>-0.256951</td>\n","      <td>-0.162718</td>\n","      <td>-0.192070</td>\n","      <td>-0.144572</td>\n","      <td>-0.157713</td>\n","      <td>-0.311888</td>\n","      <td>1.100900</td>\n","      <td>1.022524</td>\n","      <td>5.297738</td>\n","      <td>-0.019815</td>\n","      <td>-0.350218</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>128796</th>\n","      <td>-0.013662</td>\n","      <td>1.063190</td>\n","      <td>-0.768766</td>\n","      <td>-0.320240</td>\n","      <td>-0.198818</td>\n","      <td>-0.215650</td>\n","      <td>-0.256291</td>\n","      <td>-0.170982</td>\n","      <td>-0.184639</td>\n","      <td>-0.129028</td>\n","      <td>-0.162218</td>\n","      <td>-0.314453</td>\n","      <td>-0.069140</td>\n","      <td>-0.290103</td>\n","      <td>1.063190</td>\n","      <td>-0.019158</td>\n","      <td>-0.295580</td>\n","      <td>-0.016919</td>\n","    </tr>\n","    <tr>\n","      <th>128797</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.767515</td>\n","      <td>-0.327081</td>\n","      <td>0.187550</td>\n","      <td>0.768236</td>\n","      <td>0.553258</td>\n","      <td>1.152318</td>\n","      <td>0.989177</td>\n","      <td>1.093897</td>\n","      <td>-0.171887</td>\n","      <td>-0.319921</td>\n","      <td>-0.788736</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.019815</td>\n","      <td>-0.296418</td>\n","      <td>-0.017579</td>\n","    </tr>\n","    <tr>\n","      <th>128798</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.175046</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.178509</td>\n","      <td>-0.087811</td>\n","      <td>-0.171929</td>\n","      <td>-0.317914</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.021130</td>\n","      <td>-0.135435</td>\n","      <td>-0.017579</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>128799 rows Ã— 18 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c658fe50-199d-4bbb-b01a-c0ffa1596785')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c658fe50-199d-4bbb-b01a-c0ffa1596785 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c658fe50-199d-4bbb-b01a-c0ffa1596785');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["              0         1         2         3         4         5         6   \\\n","0      -0.013860 -0.002638 -0.297168  0.263236 -0.201793 -0.223605 -0.262023   \n","1      -0.013832 -0.120727  1.313647  1.129595 -0.201762 -0.223471 -0.261847   \n","2      -0.013914 -0.502245 -0.768766 -0.327081 -0.201791 -0.223640 -0.262063   \n","3      -0.013757  0.228998  1.329807  3.024606 -0.201795 -0.223588 -0.261952   \n","4      -0.013914 -0.502245 -0.768766 -0.327081 -0.201788 -0.223640 -0.262063   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","128794 -0.013662  1.063190 -0.768766 -0.313283 -0.199960 -0.217234 -0.261441   \n","128795 -0.013292  5.297738  1.913243 -0.250576 -0.199541 -0.215654 -0.256951   \n","128796 -0.013662  1.063190 -0.768766 -0.320240 -0.198818 -0.215650 -0.256291   \n","128797 -0.013914 -0.502245 -0.767515 -0.327081  0.187550  0.768236  0.553258   \n","128798 -0.013914 -0.502245 -0.768766 -0.327081 -0.175046 -0.223640 -0.262063   \n","\n","              7         8         9         10        11        12        13  \\\n","0      -0.170951 -0.191990 -0.144498  0.421145  0.523996 -0.264379 -0.344355   \n","1      -0.170926 -0.191751 -0.144392  0.133563  0.027806  1.245935  0.664355   \n","2      -0.171015 -0.192073 -0.144585  1.483008  0.622046 -0.790131 -0.391716   \n","3      -0.170967 -0.191930 -0.144500  0.517681  0.465102  1.245935  0.664182   \n","4      -0.171015 -0.192073 -0.144585  1.359169  0.551555 -0.790131 -0.391716   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","128794 -0.170975 -0.188745 -0.130657 -0.152341 -0.312547 -0.069140 -0.273168   \n","128795 -0.162718 -0.192070 -0.144572 -0.157713 -0.311888  1.100900  1.022524   \n","128796 -0.170982 -0.184639 -0.129028 -0.162218 -0.314453 -0.069140 -0.290103   \n","128797  1.152318  0.989177  1.093897 -0.171887 -0.319921 -0.788736 -0.391716   \n","128798 -0.171015 -0.178509 -0.087811 -0.171929 -0.317914 -0.790131 -0.391716   \n","\n","              14        15         16        17  \n","0      -0.002638 -0.019815  -0.340588 -0.016919  \n","1      -0.120727 -0.018500  -0.338914 -0.016919  \n","2      -0.502245 -0.020473   0.509332 -0.017579  \n","3       0.228998 -0.018500  -0.338914 -0.016919  \n","4      -0.502245 -0.020473   0.509332 -0.017579  \n","...          ...       ...        ...       ...  \n","128794  1.063190 -0.019158  13.368871 -0.016919  \n","128795  5.297738 -0.019815  -0.350218 -0.016919  \n","128796  1.063190 -0.019158  -0.295580 -0.016919  \n","128797 -0.502245 -0.019815  -0.296418 -0.017579  \n","128798 -0.502245 -0.021130  -0.135435 -0.017579  \n","\n","[128799 rows x 18 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RCOnc2AFtPA"},"outputs":[],"source":["df[18]=Y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":687,"status":"ok","timestamp":1665140796144,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"SKZG94PqF3v_","outputId":"b8a6ee32-09d3-4d7d-b1c7-b868ca91f58c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    88502\n","3    28502\n","4    11081\n","1      641\n","2       45\n","5       28\n","Name: 18, dtype: int64"]},"metadata":{},"execution_count":11}],"source":["df[18].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGqoabJrV1G_","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"ok","timestamp":1665051111771,"user_tz":-360,"elapsed":8,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"17b4a68c-6921-42c0-d714-68c8c45dfcf1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              0         1         2         3         4         5         6   \\\n","0      -0.013860 -0.002638 -0.297168  0.263236 -0.201793 -0.223605 -0.262023   \n","1      -0.013832 -0.120727  1.313647  1.129595 -0.201762 -0.223471 -0.261847   \n","2      -0.013914 -0.502245 -0.768766 -0.327081 -0.201791 -0.223640 -0.262063   \n","3      -0.013757  0.228998  1.329807  3.024606 -0.201795 -0.223588 -0.261952   \n","4      -0.013914 -0.502245 -0.768766 -0.327081 -0.201788 -0.223640 -0.262063   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","128794 -0.013662  1.063190 -0.768766 -0.313283 -0.199960 -0.217234 -0.261441   \n","128795 -0.013292  5.297738  1.913243 -0.250576 -0.199541 -0.215654 -0.256951   \n","128796 -0.013662  1.063190 -0.768766 -0.320240 -0.198818 -0.215650 -0.256291   \n","128797 -0.013914 -0.502245 -0.767515 -0.327081  0.187550  0.768236  0.553258   \n","128798 -0.013914 -0.502245 -0.768766 -0.327081 -0.175046 -0.223640 -0.262063   \n","\n","              7         8         9         10        11        12        13  \\\n","0      -0.170951 -0.191990 -0.144498  0.421145  0.523996 -0.264379 -0.344355   \n","1      -0.170926 -0.191751 -0.144392  0.133563  0.027806  1.245935  0.664355   \n","2      -0.171015 -0.192073 -0.144585  1.483008  0.622046 -0.790131 -0.391716   \n","3      -0.170967 -0.191930 -0.144500  0.517681  0.465102  1.245935  0.664182   \n","4      -0.171015 -0.192073 -0.144585  1.359169  0.551555 -0.790131 -0.391716   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","128794 -0.170975 -0.188745 -0.130657 -0.152341 -0.312547 -0.069140 -0.273168   \n","128795 -0.162718 -0.192070 -0.144572 -0.157713 -0.311888  1.100900  1.022524   \n","128796 -0.170982 -0.184639 -0.129028 -0.162218 -0.314453 -0.069140 -0.290103   \n","128797  1.152318  0.989177  1.093897 -0.171887 -0.319921 -0.788736 -0.391716   \n","128798 -0.171015 -0.178509 -0.087811 -0.171929 -0.317914 -0.790131 -0.391716   \n","\n","              14        15         16        17  18  \n","0      -0.002638 -0.019815  -0.340588 -0.016919   0  \n","1      -0.120727 -0.018500  -0.338914 -0.016919   0  \n","2      -0.502245 -0.020473   0.509332 -0.017579   0  \n","3       0.228998 -0.018500  -0.338914 -0.016919   0  \n","4      -0.502245 -0.020473   0.509332 -0.017579   0  \n","...          ...       ...        ...       ...  ..  \n","128794  1.063190 -0.019158  13.368871 -0.016919   3  \n","128795  5.297738 -0.019815  -0.350218 -0.016919   3  \n","128796  1.063190 -0.019158  -0.295580 -0.016919   3  \n","128797 -0.502245 -0.019815  -0.296418 -0.017579   3  \n","128798 -0.502245 -0.021130  -0.135435 -0.017579   3  \n","\n","[128799 rows x 19 columns]"],"text/html":["\n","  <div id=\"df-9e077b88-5bf7-4264-9196-fa0af563047f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.013860</td>\n","      <td>-0.002638</td>\n","      <td>-0.297168</td>\n","      <td>0.263236</td>\n","      <td>-0.201793</td>\n","      <td>-0.223605</td>\n","      <td>-0.262023</td>\n","      <td>-0.170951</td>\n","      <td>-0.191990</td>\n","      <td>-0.144498</td>\n","      <td>0.421145</td>\n","      <td>0.523996</td>\n","      <td>-0.264379</td>\n","      <td>-0.344355</td>\n","      <td>-0.002638</td>\n","      <td>-0.019815</td>\n","      <td>-0.340588</td>\n","      <td>-0.016919</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.013832</td>\n","      <td>-0.120727</td>\n","      <td>1.313647</td>\n","      <td>1.129595</td>\n","      <td>-0.201762</td>\n","      <td>-0.223471</td>\n","      <td>-0.261847</td>\n","      <td>-0.170926</td>\n","      <td>-0.191751</td>\n","      <td>-0.144392</td>\n","      <td>0.133563</td>\n","      <td>0.027806</td>\n","      <td>1.245935</td>\n","      <td>0.664355</td>\n","      <td>-0.120727</td>\n","      <td>-0.018500</td>\n","      <td>-0.338914</td>\n","      <td>-0.016919</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.201791</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.192073</td>\n","      <td>-0.144585</td>\n","      <td>1.483008</td>\n","      <td>0.622046</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.020473</td>\n","      <td>0.509332</td>\n","      <td>-0.017579</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.013757</td>\n","      <td>0.228998</td>\n","      <td>1.329807</td>\n","      <td>3.024606</td>\n","      <td>-0.201795</td>\n","      <td>-0.223588</td>\n","      <td>-0.261952</td>\n","      <td>-0.170967</td>\n","      <td>-0.191930</td>\n","      <td>-0.144500</td>\n","      <td>0.517681</td>\n","      <td>0.465102</td>\n","      <td>1.245935</td>\n","      <td>0.664182</td>\n","      <td>0.228998</td>\n","      <td>-0.018500</td>\n","      <td>-0.338914</td>\n","      <td>-0.016919</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.201788</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.192073</td>\n","      <td>-0.144585</td>\n","      <td>1.359169</td>\n","      <td>0.551555</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.020473</td>\n","      <td>0.509332</td>\n","      <td>-0.017579</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128794</th>\n","      <td>-0.013662</td>\n","      <td>1.063190</td>\n","      <td>-0.768766</td>\n","      <td>-0.313283</td>\n","      <td>-0.199960</td>\n","      <td>-0.217234</td>\n","      <td>-0.261441</td>\n","      <td>-0.170975</td>\n","      <td>-0.188745</td>\n","      <td>-0.130657</td>\n","      <td>-0.152341</td>\n","      <td>-0.312547</td>\n","      <td>-0.069140</td>\n","      <td>-0.273168</td>\n","      <td>1.063190</td>\n","      <td>-0.019158</td>\n","      <td>13.368871</td>\n","      <td>-0.016919</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>128795</th>\n","      <td>-0.013292</td>\n","      <td>5.297738</td>\n","      <td>1.913243</td>\n","      <td>-0.250576</td>\n","      <td>-0.199541</td>\n","      <td>-0.215654</td>\n","      <td>-0.256951</td>\n","      <td>-0.162718</td>\n","      <td>-0.192070</td>\n","      <td>-0.144572</td>\n","      <td>-0.157713</td>\n","      <td>-0.311888</td>\n","      <td>1.100900</td>\n","      <td>1.022524</td>\n","      <td>5.297738</td>\n","      <td>-0.019815</td>\n","      <td>-0.350218</td>\n","      <td>-0.016919</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>128796</th>\n","      <td>-0.013662</td>\n","      <td>1.063190</td>\n","      <td>-0.768766</td>\n","      <td>-0.320240</td>\n","      <td>-0.198818</td>\n","      <td>-0.215650</td>\n","      <td>-0.256291</td>\n","      <td>-0.170982</td>\n","      <td>-0.184639</td>\n","      <td>-0.129028</td>\n","      <td>-0.162218</td>\n","      <td>-0.314453</td>\n","      <td>-0.069140</td>\n","      <td>-0.290103</td>\n","      <td>1.063190</td>\n","      <td>-0.019158</td>\n","      <td>-0.295580</td>\n","      <td>-0.016919</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>128797</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.767515</td>\n","      <td>-0.327081</td>\n","      <td>0.187550</td>\n","      <td>0.768236</td>\n","      <td>0.553258</td>\n","      <td>1.152318</td>\n","      <td>0.989177</td>\n","      <td>1.093897</td>\n","      <td>-0.171887</td>\n","      <td>-0.319921</td>\n","      <td>-0.788736</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.019815</td>\n","      <td>-0.296418</td>\n","      <td>-0.017579</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>128798</th>\n","      <td>-0.013914</td>\n","      <td>-0.502245</td>\n","      <td>-0.768766</td>\n","      <td>-0.327081</td>\n","      <td>-0.175046</td>\n","      <td>-0.223640</td>\n","      <td>-0.262063</td>\n","      <td>-0.171015</td>\n","      <td>-0.178509</td>\n","      <td>-0.087811</td>\n","      <td>-0.171929</td>\n","      <td>-0.317914</td>\n","      <td>-0.790131</td>\n","      <td>-0.391716</td>\n","      <td>-0.502245</td>\n","      <td>-0.021130</td>\n","      <td>-0.135435</td>\n","      <td>-0.017579</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>128799 rows Ã— 19 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e077b88-5bf7-4264-9196-fa0af563047f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9e077b88-5bf7-4264-9196-fa0af563047f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9e077b88-5bf7-4264-9196-fa0af563047f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qa2bkK3l4B2l"},"outputs":[],"source":["y_train=traindata[66].values\n","y_test=testdata[66].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIo2n5bVH9Iy"},"outputs":[],"source":["traindata, testdata = train_test_split(df,test_size=0.20,random_state=42)"]},{"cell_type":"code","source":["traindata[18].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKtvq_qHITLa","executionInfo":{"status":"ok","timestamp":1665140887568,"user_tz":-360,"elapsed":5,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"45860f53-b5f6-44dc-a46d-dd5b23a8092d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    70642\n","3    22900\n","4     8916\n","1      523\n","2       35\n","5       23\n","Name: 18, dtype: int64"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["testdata[18].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpAR9cPdIdpE","executionInfo":{"status":"ok","timestamp":1665140920251,"user_tz":-360,"elapsed":433,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"23fe037b-9a79-4a67-d3ce-479212ab0e66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    17860\n","3     5602\n","4     2165\n","1      118\n","2       10\n","5        5\n","Name: 18, dtype: int64"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["df.to_csv(r'mscad_std.csv')"],"metadata":{"id":"K3DRCPPg5PAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZONuiG6GU6K"},"outputs":[],"source":["trainX=traindata.iloc[:,0:18].values\n","testX=testdata.iloc[:,0:18].values\n","\n","y_train=to_categorical(traindata[18])\n","y_test=to_categorical(testdata[18])"]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","rf=RandomForestClassifier()\n","dt=DecisionTreeClassifier()"],"metadata":{"id":"X5ktVa9gKMJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check each classifier accuracy here\n","clf=rf\n","clf.fit(trainX,y_train)\n","y_pred=clf.predict(testX)\n","acc=accuracy_score(y_test,y_pred)\n","prc=precision_score(y_test,y_pred, average=\"weighted\")\n","rc=recall_score(y_test,y_pred, average=\"weighted\")\n","f1=f1_score(y_test,y_pred, average=\"weighted\")\n","print(rf)\n","print('Accuracy: ',acc, '\\nPrecision: ' ,prc, '\\nRecall: ', rc,'\\nF1-Score: ', f1)\n","print(acc)\n"],"metadata":{"id":"nSR1ITXxKje9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(metrics.classification_report(y_test,y_pred))\n","cm=metrics.confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n","print(cm)"],"metadata":{"id":"8j0w1hXZLOVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","ROC_AUC_SCORE=roc_auc_score(y_test, y_pred,multi_class='ovr')\n","print('ROC_AUC_SCORE : ',ROC_AUC_SCORE)"],"metadata":{"id":"0tSuRd9MV-gs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJHisH8nq9Np"},"outputs":[],"source":["import keras.backend as K\n","from keras.layers import Layer\n","from keras.layers import Input\n","from keras import initializers, regularizers, constraints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__9i3IxEhNcx"},"outputs":[],"source":["# Add attention layer to the deep learning network\n","class attention(Layer):\n","    def __init__(self,**kwargs):\n","        super(attention,self).__init__(**kwargs)\n"," \n","    def build(self,input_shape):\n","        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n","                               initializer='random_normal', trainable=True)\n","        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n","                               initializer='zeros', trainable=True)        \n","        super(attention, self).build(input_shape)\n"," \n","    def call(self,x):\n","        # Alignment scores. Pass them through tanh function\n","        e = K.tanh(K.dot(x,self.W)+self.b)\n","        # Remove dimension of size 1\n","        e = K.squeeze(e, axis=-1)   \n","        # Compute the weights\n","        alpha = K.softmax(e)\n","        # Reshape to tensorFlow format\n","        alpha = K.expand_dims(alpha, axis=-1)\n","        # Compute the context vector\n","        context = x * alpha\n","        context = K.sum(context, axis=1)\n","        return context"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1663867361659,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"RAauz0TP5Lmj","outputId":"49217bc8-85e3-4d9a-9f94-3bdfc7fe4d21"},"outputs":[{"data":{"text/plain":["array([[1., 0., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0., 0.],\n","       [1., 0., 0., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 1., 0., 0.]], dtype=float32)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":398,"status":"ok","timestamp":1665071823987,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"YcVBOAhS482h","outputId":"5088c76e-c399-4863-b52d-9cbd63480489"},"outputs":[{"output_type":"stream","name":"stdout","text":["(103039, 18, 1)\n"]}],"source":["# reshape input to be [samples, time steps, features]\n","X_train = np.reshape(trainX, (trainX.shape[0], trainX.shape[1],1))\n","X_test=np.reshape(testX, (testX.shape[0], testX.shape[1],1))\n","\n","print(X_train.shape)\n","\n","\n","batch_size = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1387,"status":"ok","timestamp":1665071832732,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"j-f331Ry5PVh","outputId":"5b922277-921f-44cd-dc9e-a58392e6934d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 18, 64)            16896     \n","                                                                 \n"," attention (attention)       (None, 64)                82        \n","                                                                 \n"," dense (Dense)               (None, 32)                2080      \n","                                                                 \n"," dense_1 (Dense)             (None, 6)                 198       \n","                                                                 \n","=================================================================\n","Total params: 19,256\n","Trainable params: 19,256\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","model.add(LSTM(64,return_sequences=True, input_shape=(18,1)))\n","model.add(attention())\n","model.add(Dense(32,activation='relu'))\n","model.add(Dense(6, activation=\"softmax\"))\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4PnoOs5m6oT","executionInfo":{"status":"ok","timestamp":1665083855261,"user_tz":-360,"elapsed":2602413,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"0aa7ef0a-5dc8-44a2-f3a4-07dab965e3bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n","Epoch 1: val_accuracy improved from -inf to 0.99720, saving model to kddresults/nslkddmc/checkpoint-01.hdf5\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0192 - val_accuracy: 0.9972\n","Epoch 2/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 2: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 28s 17ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0189 - val_accuracy: 0.9971\n","Epoch 3/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9981\n","Epoch 3: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0187 - val_accuracy: 0.9970\n","Epoch 4/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 4: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0203 - val_accuracy: 0.9970\n","Epoch 5/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 5: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0205 - val_accuracy: 0.9970\n","Epoch 6/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 6: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0208 - val_accuracy: 0.9967\n","Epoch 7/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 7: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0215 - val_accuracy: 0.9971\n","Epoch 8/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 8: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0207 - val_accuracy: 0.9967\n","Epoch 9/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 9: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0209 - val_accuracy: 0.9970\n","Epoch 10/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 10: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0226 - val_accuracy: 0.9970\n","Epoch 11/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n","Epoch 11: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0217 - val_accuracy: 0.9969\n","Epoch 12/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 12: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0192 - val_accuracy: 0.9971\n","Epoch 13/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9982\n","Epoch 13: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0199 - val_accuracy: 0.9969\n","Epoch 14/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9979\n","Epoch 14: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0194 - val_accuracy: 0.9969\n","Epoch 15/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 15: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0201 - val_accuracy: 0.9969\n","Epoch 16/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 16: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0189 - val_accuracy: 0.9970\n","Epoch 17/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 17: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0201 - val_accuracy: 0.9971\n","Epoch 18/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 18: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0203 - val_accuracy: 0.9970\n","Epoch 19/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 19: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0215 - val_accuracy: 0.9970\n","Epoch 20/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 20: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 24s 15ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0206 - val_accuracy: 0.9969\n","Epoch 21/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 21: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0208 - val_accuracy: 0.9969\n","Epoch 22/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 22: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0207 - val_accuracy: 0.9970\n","Epoch 23/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 23: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0202 - val_accuracy: 0.9970\n","Epoch 24/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 24: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0203 - val_accuracy: 0.9971\n","Epoch 25/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9980\n","Epoch 25: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0213 - val_accuracy: 0.9967\n","Epoch 26/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 26: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0233 - val_accuracy: 0.9965\n","Epoch 27/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9981\n","Epoch 27: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.0205 - val_accuracy: 0.9968\n","Epoch 28/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9981\n","Epoch 28: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0202 - val_accuracy: 0.9971\n","Epoch 29/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 29: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0211 - val_accuracy: 0.9970\n","Epoch 30/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 30: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0225 - val_accuracy: 0.9969\n","Epoch 31/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 31: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 24s 15ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0205 - val_accuracy: 0.9969\n","Epoch 32/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 32: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0224 - val_accuracy: 0.9969\n","Epoch 33/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9980\n","Epoch 33: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.0230 - val_accuracy: 0.9969\n","Epoch 34/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9980\n","Epoch 34: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 0.0229 - val_accuracy: 0.9970\n","Epoch 35/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 35: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0227 - val_accuracy: 0.9965\n","Epoch 36/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 36: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0216 - val_accuracy: 0.9970\n","Epoch 37/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 37: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0207 - val_accuracy: 0.9970\n","Epoch 38/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 38: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0203 - val_accuracy: 0.9970\n","Epoch 39/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9980\n","Epoch 39: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0217 - val_accuracy: 0.9967\n","Epoch 40/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 40: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0222 - val_accuracy: 0.9968\n","Epoch 41/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 41: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0202 - val_accuracy: 0.9971\n","Epoch 42/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 42: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0210 - val_accuracy: 0.9969\n","Epoch 43/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 43: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0200 - val_accuracy: 0.9969\n","Epoch 44/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 44: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0221 - val_accuracy: 0.9967\n","Epoch 45/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 45: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0229 - val_accuracy: 0.9967\n","Epoch 46/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 46: val_accuracy did not improve from 0.99720\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0212 - val_accuracy: 0.9971\n","Epoch 47/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 47: val_accuracy improved from 0.99720 to 0.99732, saving model to kddresults/nslkddmc/checkpoint-47.hdf5\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0203 - val_accuracy: 0.9973\n","Epoch 48/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 48: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 24s 15ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0225 - val_accuracy: 0.9969\n","Epoch 49/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 49: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0222 - val_accuracy: 0.9967\n","Epoch 50/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9980\n","Epoch 50: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.0223 - val_accuracy: 0.9970\n","Epoch 51/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 51: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0227 - val_accuracy: 0.9967\n","Epoch 52/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 52: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0228 - val_accuracy: 0.9972\n","Epoch 53/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 53: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0208 - val_accuracy: 0.9971\n","Epoch 54/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 54: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0209 - val_accuracy: 0.9971\n","Epoch 55/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 55: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 0.9971\n","Epoch 56/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 56: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0226 - val_accuracy: 0.9969\n","Epoch 57/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9981\n","Epoch 57: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0227 - val_accuracy: 0.9966\n","Epoch 58/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 58: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0212 - val_accuracy: 0.9972\n","Epoch 59/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 59: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0216 - val_accuracy: 0.9972\n","Epoch 60/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 60: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0214 - val_accuracy: 0.9970\n","Epoch 61/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 61: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0211 - val_accuracy: 0.9970\n","Epoch 62/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9980\n","Epoch 62: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.0234 - val_accuracy: 0.9968\n","Epoch 63/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9980\n","Epoch 63: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.0213 - val_accuracy: 0.9970\n","Epoch 64/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 64: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0216 - val_accuracy: 0.9970\n","Epoch 65/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9982\n","Epoch 65: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0224 - val_accuracy: 0.9965\n","Epoch 66/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n","Epoch 66: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0217 - val_accuracy: 0.9968\n","Epoch 67/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 67: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 24s 15ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0229 - val_accuracy: 0.9970\n","Epoch 68/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 68: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0216 - val_accuracy: 0.9971\n","Epoch 69/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 69: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 0.9971\n","Epoch 70/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 70: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 0.9970\n","Epoch 71/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 71: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0210 - val_accuracy: 0.9972\n","Epoch 72/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 72: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0232 - val_accuracy: 0.9969\n","Epoch 73/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 73: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0220 - val_accuracy: 0.9972\n","Epoch 74/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9980\n","Epoch 74: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.0218 - val_accuracy: 0.9968\n","Epoch 75/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9982\n","Epoch 75: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0239 - val_accuracy: 0.9969\n","Epoch 76/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 76: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0234 - val_accuracy: 0.9969\n","Epoch 77/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9983\n","Epoch 77: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0235 - val_accuracy: 0.9968\n","Epoch 78/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 78: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 16ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0242 - val_accuracy: 0.9969\n","Epoch 79/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 79: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 28s 17ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0255 - val_accuracy: 0.9970\n","Epoch 80/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9982\n","Epoch 80: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0230 - val_accuracy: 0.9970\n","Epoch 81/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9982\n","Epoch 81: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0230 - val_accuracy: 0.9972\n","Epoch 82/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 82: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0210 - val_accuracy: 0.9971\n","Epoch 83/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9982\n","Epoch 83: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.0223 - val_accuracy: 0.9971\n","Epoch 84/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 84: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0230 - val_accuracy: 0.9969\n","Epoch 85/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 85: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0230 - val_accuracy: 0.9970\n","Epoch 86/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9983\n","Epoch 86: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 28s 18ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0244 - val_accuracy: 0.9971\n","Epoch 87/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9980\n","Epoch 87: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.0247 - val_accuracy: 0.9964\n","Epoch 88/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 88: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0244 - val_accuracy: 0.9968\n","Epoch 89/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9983\n","Epoch 89: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.0245 - val_accuracy: 0.9968\n","Epoch 90/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 90: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0254 - val_accuracy: 0.9966\n","Epoch 91/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9983\n","Epoch 91: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.0252 - val_accuracy: 0.9969\n","Epoch 92/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 92: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0241 - val_accuracy: 0.9969\n","Epoch 93/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 93: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0249 - val_accuracy: 0.9968\n","Epoch 94/100\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 94: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0240 - val_accuracy: 0.9969\n","Epoch 95/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 95: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0232 - val_accuracy: 0.9969\n","Epoch 96/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 96: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0224 - val_accuracy: 0.9970\n","Epoch 97/100\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 97: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0237 - val_accuracy: 0.9969\n","Epoch 98/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 98: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0233 - val_accuracy: 0.9970\n","Epoch 99/100\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 99: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0259 - val_accuracy: 0.9961\n","Epoch 100/100\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 100: val_accuracy did not improve from 0.99732\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0244 - val_accuracy: 0.9970\n","CPU times: user 1h 4min, sys: 3min 11s, total: 1h 7min 12s\n","Wall time: 43min 21s\n"]}],"source":["%%time\n","checkpointer = callbacks.ModelCheckpoint(filepath=\"model18.hdf5\", verbose=1, save_best_only=True, monitor='val_accuracy',mode='max')\n","csv_logger = CSVLogger('training_set_iranalysis.csv',separator=',', append=False)\n","m=model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test,y_test), callbacks=[checkpointer,csv_logger])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1282472,"status":"ok","timestamp":1665089186500,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"SpRf-EGw5v0D","outputId":"67e56a5a-f6a9-4e40-97a2-d6cf7fe9c7aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 1: val_accuracy improved from -inf to 0.99709, saving model to kddresults/nslkddmc2/checkpoint-01.hdf5\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.0214 - val_accuracy: 0.9971\n","Epoch 2/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 2: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0215 - val_accuracy: 0.9969\n","Epoch 3/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 3: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0223 - val_accuracy: 0.9969\n","Epoch 4/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 4: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0224 - val_accuracy: 0.9969\n","Epoch 5/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 5: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0235 - val_accuracy: 0.9968\n","Epoch 6/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 6: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0222 - val_accuracy: 0.9969\n","Epoch 7/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 7: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0221 - val_accuracy: 0.9969\n","Epoch 8/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 8: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0205 - val_accuracy: 0.9968\n","Epoch 9/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9980\n","Epoch 9: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 24s 15ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.0228 - val_accuracy: 0.9969\n","Epoch 10/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 10: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0207 - val_accuracy: 0.9967\n","Epoch 11/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9983\n","Epoch 11: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.0221 - val_accuracy: 0.9968\n","Epoch 12/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 12: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0228 - val_accuracy: 0.9966\n","Epoch 13/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 13: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0211 - val_accuracy: 0.9970\n","Epoch 14/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 14: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0201 - val_accuracy: 0.9970\n","Epoch 15/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 15: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0211 - val_accuracy: 0.9970\n","Epoch 16/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 16: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0220 - val_accuracy: 0.9971\n","Epoch 17/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 17: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0227 - val_accuracy: 0.9970\n","Epoch 18/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 18: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0233 - val_accuracy: 0.9970\n","Epoch 19/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 19: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0245 - val_accuracy: 0.9969\n","Epoch 20/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 20: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0273 - val_accuracy: 0.9960\n","Epoch 21/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 21: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0216 - val_accuracy: 0.9966\n","Epoch 22/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 22: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0263 - val_accuracy: 0.9970\n","Epoch 23/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 23: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0206 - val_accuracy: 0.9970\n","Epoch 24/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 24: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0232 - val_accuracy: 0.9970\n","Epoch 25/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 25: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0222 - val_accuracy: 0.9968\n","Epoch 26/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9982\n","Epoch 26: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0232 - val_accuracy: 0.9964\n","Epoch 27/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 27: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 15ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0215 - val_accuracy: 0.9969\n","Epoch 28/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9982\n","Epoch 28: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.0216 - val_accuracy: 0.9970\n","Epoch 29/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9981\n","Epoch 29: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0220 - val_accuracy: 0.9969\n","Epoch 30/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9981\n","Epoch 30: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.0224 - val_accuracy: 0.9968\n","Epoch 31/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 31: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0078 - accuracy: 0.9981 - val_loss: 0.0212 - val_accuracy: 0.9968\n","Epoch 32/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9983\n","Epoch 32: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.0204 - val_accuracy: 0.9971\n","Epoch 33/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 33: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0235 - val_accuracy: 0.9967\n","Epoch 34/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 34: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0217 - val_accuracy: 0.9969\n","Epoch 35/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 35: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0225 - val_accuracy: 0.9969\n","Epoch 36/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n","Epoch 36: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0204 - val_accuracy: 0.9969\n","Epoch 37/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 37: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0205 - val_accuracy: 0.9969\n","Epoch 38/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 38: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0212 - val_accuracy: 0.9969\n","Epoch 39/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9981\n","Epoch 39: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.0223 - val_accuracy: 0.9970\n","Epoch 40/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n","Epoch 40: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0222 - val_accuracy: 0.9970\n","Epoch 41/50\n","1610/1610 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981\n","Epoch 41: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0225 - val_accuracy: 0.9969\n","Epoch 42/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9982\n","Epoch 42: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.0228 - val_accuracy: 0.9970\n","Epoch 43/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9983\n","Epoch 43: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0232 - val_accuracy: 0.9970\n","Epoch 44/50\n","1607/1610 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9981\n","Epoch 44: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0234 - val_accuracy: 0.9968\n","Epoch 45/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 45: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0243 - val_accuracy: 0.9967\n","Epoch 46/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9982\n","Epoch 46: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0225 - val_accuracy: 0.9969\n","Epoch 47/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9981\n","Epoch 47: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 27s 17ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.0247 - val_accuracy: 0.9971\n","Epoch 48/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9981\n","Epoch 48: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0244 - val_accuracy: 0.9970\n","Epoch 49/50\n","1609/1610 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9982\n","Epoch 49: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 25s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0256 - val_accuracy: 0.9969\n","Epoch 50/50\n","1608/1610 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9982\n","Epoch 50: val_accuracy did not improve from 0.99709\n","1610/1610 [==============================] - 26s 16ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0225 - val_accuracy: 0.9970\n","CPU times: user 31min 57s, sys: 1min 39s, total: 33min 36s\n","Wall time: 21min 22s\n"]}],"source":["%%time\n","checkpointer = callbacks.ModelCheckpoint(filepath=\"kddresults/nslkddmc2/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_accuracy',mode='max')\n","csv_logger = CSVLogger('training_set_iranalysis2.csv',separator=',', append=False)\n","m1=model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test,y_test), callbacks=[checkpointer,csv_logger])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNF9e6gW2CWx"},"outputs":[],"source":["model.load_weights('/content/kddresults/nslkddmc/checkpoint-47.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4105,"status":"ok","timestamp":1665086538735,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"ohwk7eLO6Ai6","outputId":"16c26b97-8cfd-4d5e-d9d1-02dc22a04f71"},"outputs":[{"output_type":"stream","name":"stdout","text":["805/805 [==============================] - 4s 5ms/step - loss: 0.0203 - accuracy: 0.9973\n","\n","Loss: 0.020, Accuracy: 99.732%\n"]}],"source":["\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(\"\\nLoss: %.3f, Accuracy: %.3f%%\" % (loss, accuracy*100))\n","#y_pred = model.predict_classes(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jzg3ri6ZNSxu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665086780660,"user_tz":-360,"elapsed":3759,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"5da10373-d3e4-46c8-db5a-9650e44d43ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.78 s, sys: 215 ms, total: 3.99 s\n","Wall time: 3.34 s\n"]}],"source":["%%time\n","y_pred=model.predict(X_test) \n","classes_x=np.argmax(y_pred,axis=1)\n","class_x=np.argmax(y_pred.round(),axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZWRTMakR8-u"},"outputs":[],"source":["y_true=np.argmax(y_test,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1665086852116,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"pGzw3Q3PRqcu","outputId":"1d8c757f-5198-4a3c-e63c-8ce58520312c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["recall = recall_score(y_true, classes_x , average=\"weighted\")\n","precision = precision_score(y_true, classes_x , average=\"weighted\")\n","f1= f1_score(y_true, classes_x , average=\"weighted\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1665086855564,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"olaxT7fDky0C","outputId":"a6a8025c-7e28-49f3-d724-ba613411604a"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     17860\n","           1       0.91      0.96      0.93       118\n","           2       1.00      0.70      0.82        10\n","           3       1.00      1.00      1.00      5602\n","           4       0.99      0.99      0.99      2165\n","           5       0.00      0.00      0.00         5\n","\n","    accuracy                           1.00     25760\n","   macro avg       0.82      0.77      0.79     25760\n","weighted avg       1.00      1.00      1.00     25760\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_true,classes_x))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1665086862561,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"ibh5uX5URnK3","outputId":"0e6a374b-5f5f-49bb-cc36-159d42d5b1cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["confusion matrix\n","----------------------------------------------\n","accuracy\n","0.997321\n","recall\n","0.997321\n","precision\n","0.997144\n","f1score\n","0.997219\n","==============================================\n","[[17854     0     0     4     2     0]\n"," [    1   113     0     3     1     0]\n"," [    0     0     7     2     1     0]\n"," [    3     0     0  5584    15     0]\n"," [   16    11     0     5  2133     0]\n"," [    0     0     0     2     3     0]]\n"]}],"source":["\n","print(\"confusion matrix\")\n","print(\"----------------------------------------------\")\n","print(\"accuracy\")\n","print(\"%.6f\" %accuracy)\n","print(\"recall\")\n","print(\"%.6f\" %recall)\n","print(\"precision\")\n","print(\"%.6f\" %precision)\n","print(\"f1score\")\n","print(\"%.6f\" %f1)\n","cm = metrics.confusion_matrix(y_true, classes_x)\n","print(\"==============================================\")\n","print (cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":513,"status":"ok","timestamp":1664027651903,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"},"user_tz":-360},"id":"6SPPSSORSh-E","outputId":"768d0c74-b3e0-45ee-bd7c-0bece2e892f2"},"outputs":[{"data":{"text/plain":["0    17860\n","3     5602\n","4     2165\n","1      118\n","2       10\n","5        5\n","Name: 18, dtype: int64"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["testdata[testdata.columns[18]].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS7L00gtxbtO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665064742237,"user_tz":-360,"elapsed":469,"user":{"displayName":"Kowshik Sankar Roy","userId":"07567045859000563067"}},"outputId":"c4b8b6c6-0347-41f2-f421-d6fafbd62071"},"outputs":[{"output_type":"stream","name":"stdout","text":["confusion matrix\n","----------------------------------------------\n","accuracy\n","0.996933\n","recall\n","0.996933\n","precision\n","0.996877\n","f1score\n","0.996888\n","==============================================\n","[[17854     0     0     5     1     0]\n"," [    1   113     0     3     1     0]\n"," [    0     0     7     3     0     0]\n"," [    6     0     0  5581    12     3]\n"," [   17    12     0    10  2126     0]\n"," [    0     0     0     4     1     0]]\n"]}],"source":["\n","print(\"confusion matrix\")\n","print(\"----------------------------------------------\")\n","print(\"accuracy\")\n","print(\"%.6f\" %accuracy)\n","print(\"recall\")\n","print(\"%.6f\" %recall)\n","print(\"precision\")\n","print(\"%.6f\" %precision)\n","print(\"f1score\")\n","print(\"%.6f\" %f1)\n","cm = metrics.confusion_matrix(y_true, classes_x)\n","print(\"==============================================\")\n","print (cm)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBlwSmrlmVJufRpV4IBiK8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}